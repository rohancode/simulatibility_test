{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "# pandas version: 0.25.1\n",
    "\n",
    "import numpy as np\n",
    "# numpy version: 1.16.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "# keras version: 2.2.4\n",
    "# Using TensorFlow backend in Keras (1.13.1)\n",
    "\n",
    "import innvestigate\n",
    "# innvestigate version: 1.0.8\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import ngrams, FreqDist\n",
    "# nltk version: 3.4.5\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "# lime version: 0.2.0.1\n",
    "\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "base_folder = '../aclImdb/'\n",
    "\n",
    "def parse_folder(name):\n",
    "    data = []\n",
    "    for verdict in ('neg', 'pos'):\n",
    "        for file in tqdm(glob(os.path.join(name, verdict, '*.txt'))):\n",
    "            data.append({\n",
    "                'text': open(file, encoding='utf8').read(),\n",
    "                'verdict': verdict\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = parse_folder(base_folder+'train/')\n",
    "df_test = parse_folder(base_folder+'test/')\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)\n",
    "df = df.sample(frac=1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df['text'])\n",
    "sentiment = list(df['verdict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, sentiment_train, sentiment_test = train_test_split(sentences, sentiment, test_size=0.20, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos:1, neg:0\n",
    "y_train = []\n",
    "for x in sentiment_train:\n",
    "    if x == 'pos':\n",
    "        y_train.append(1)\n",
    "    else:\n",
    "        y_train.append(0)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_test = []\n",
    "for x in sentiment_test:\n",
    "    if x == 'pos':\n",
    "        y_test.append(1)\n",
    "    else:\n",
    "        y_test.append(0)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    sentence = re.sub('[^A-Za-z0-9]', ' ', sen)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence_clean = \"\"\n",
    "    \n",
    "    for w in word_tokenize(sentence):\n",
    "        if (w not in stop_words) & (len(w)>2):\n",
    "            sentence_clean = sentence_clean + \" \" + ps.stem(w)\n",
    "    return sentence_clean.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocess_train = []\n",
    "for sen in tqdm(sentences_train):\n",
    "    X_preprocess_train.append(preprocess_text(sen))\n",
    "    \n",
    "X_preprocess_test = []\n",
    "for sen in tqdm(sentences_test):\n",
    "    X_preprocess_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_vectorized_train = vectorizer.fit_transform(X_preprocess_train)\n",
    "X_vectorized_test = vectorizer.transform(X_preprocess_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = list(vectorizer.vocabulary_.keys())\n",
    "vocab_idx = list(vectorizer.vocabulary_.values())\n",
    "vocab_inv = {}\n",
    "for i, idx in enumerate(vocab_idx):\n",
    "    vocab_inv[idx] = vocab_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Dense(100, activation='relu', input_shape=(500,)),\n",
    "  Dense(30, activation='relu'),\n",
    "  Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_vectorized_train, y_train , epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_vectorized_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_vectorized_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initilizing XAI Techniques</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_fn(text_list):\n",
    "    \n",
    "    X_preprocess = []\n",
    "    for sen in tqdm(text_list):\n",
    "        X_preprocess.append(preprocess_text(sen))\n",
    "    \n",
    "    X_vectorized = vectorizer.transform(X_preprocess)\n",
    "    proba = model.predict_proba(X_vectorized)\n",
    "    proba_not = 1-proba\n",
    "    proba = np.concatenate((proba, proba_not), axis=1)\n",
    "\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innvestigate_method = 'lrp.z'\n",
    "analyzer = innvestigate.create_analyzer(innvestigate_method, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case Selection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "random_cases = list(np.random.random_integers(0,9999,size=[500,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data_idx = list(range(0,len(random_cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_no = random_cases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test[case_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data_text = []\n",
    "for case_no_ in random_cases:\n",
    "    experimental_data_text.append(sentences_test[case_no_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True:', sentiment_test[case_no])\n",
    "print('Predicted:', model.predict_classes(X_vectorized_test[case_no]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_vectorized_test[case_no])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data_y = []\n",
    "for case_no_ in random_cases:\n",
    "    experimental_data_y.append(sentiment_test[case_no_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data_y_hat = []\n",
    "for case_no_ in random_cases:\n",
    "    p = model.predict_classes(X_vectorized_test[case_no_])[0][0]\n",
    "    if p == 1:\n",
    "        experimental_data_y_hat.append('pos')\n",
    "    else:\n",
    "        experimental_data_y_hat.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LIME</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(sentences_test[case_no], classifier_fn=predict_proba_fn,num_features=10)\n",
    "a = exp.as_list()\n",
    "\n",
    "pos_score = []\n",
    "pos_words = []\n",
    "neg_score = []\n",
    "neg_words = []\n",
    "for j in a:\n",
    "    if j[1] <0:\n",
    "        pos_score.append(round(j[1]*-1,5))\n",
    "        pos_words.append(j[0])\n",
    "    else:\n",
    "        neg_score.append(round(j[1]*-1,5))\n",
    "        neg_words.append(j[0])\n",
    "\n",
    "print(\"\\nPOS WORD CONTRIBUTE SCORE:-\")\n",
    "for i, score in enumerate(pos_score):\n",
    "    print(pos_words[i],':', score)\n",
    "    \n",
    "print(\"\\nNEG WORD CONTRIBUTE SCORE:-\")\n",
    "for i, score in enumerate(neg_score):\n",
    "    print(neg_words[i],':', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_highlighter_pos(p, ma, mi):\n",
    "    w = p[0]\n",
    "    s = p[1]\n",
    "    cl = ma-mi\n",
    "    g_ = (s-mi)/cl\n",
    "    c_ = ((1-g_)*50)+45\n",
    "    color =  \"hsl(133, 100%, \"+str(c_)+\"%)\"\n",
    "    word = '<span style=\"background-color:' +color+ '\">' +w+ '</span>'\n",
    "    return word\n",
    "\n",
    "text_pos = ' '.join([lime_highlighter_pos(p, max(pos_score),min(pos_score)) for p in zip(pos_words, pos_score)])\n",
    "display(HTML(text_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_highlighter_neg(p, ma, mi):\n",
    "    w = p[0]\n",
    "    s = p[1]\n",
    "    cl = ma-mi\n",
    "    g_ = (s-mi)/cl\n",
    "    c_ = ((1-g_)*50)+45\n",
    "    color =  \"hsl(0, 100%, \"+str(c_)+\"%)\"\n",
    "    word = '<span style=\"background-color:' +color+ '\">' +w+ '</span>'\n",
    "    return word\n",
    "\n",
    "text_neg = ' '.join([lime_highlighter_neg(p, max(list(np.abs(neg_score))),min(list(np.abs(neg_score)))) for p in zip(neg_words, list(np.abs(neg_score)))])\n",
    "display(HTML(text_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_lime(case_no):\n",
    "    exp = explainer.explain_instance(sentences_test[case_no], classifier_fn=predict_proba_fn,num_features=10)\n",
    "    a = exp.as_list()\n",
    "\n",
    "    pos_score = []\n",
    "    pos_words = []\n",
    "    neg_score = []\n",
    "    neg_words = []\n",
    "    for j in a:\n",
    "        if j[1] <0:\n",
    "            pos_score.append(round(j[1]*-1,5))\n",
    "            pos_words.append(j[0])\n",
    "        else:\n",
    "            neg_score.append(round(j[1]*-1,5))\n",
    "            neg_words.append(j[0])\n",
    "\n",
    "    print(\"POS WORD CONTRIBUTE:\")\n",
    "    text_pos = ' '.join([lime_highlighter_pos(p, max(pos_score),min(pos_score)) for p in zip(pos_words, pos_score)])\n",
    "    display(HTML(text_pos))\n",
    "    \n",
    "    print(\"NEG WORD CONTRIBUTE:\")\n",
    "    text_neg = ' '.join([lime_highlighter_neg(p, max(list(np.abs(neg_score))),min(list(np.abs(neg_score)))) for p in zip(neg_words, list(np.abs(neg_score)))])\n",
    "    display(HTML(text_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cn = -1\n",
    "experimental_data_e_lime = []\n",
    "for case_no_ in tqdm(random_cases):\n",
    "    cn =cn+1\n",
    "    if (cn>=300) & (cn<=319):\n",
    "        experimental_data_e_lime.append(e_lime(case_no_))\n",
    "        print('id:', cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LRP</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case = X_vectorized_test[case_no]\n",
    "scores = np.squeeze(analyzer.analyze(case.toarray()))\n",
    "\n",
    "print(\"\\nWORD CONTRIBUTE SCORE:-\")\n",
    "top_set = []\n",
    "scores = np.abs(scores)\n",
    "ids = np.flip(scores.argsort()[-10:])\n",
    "words_list = []\n",
    "scores_list = []\n",
    "for i in ids:\n",
    "    print(vocab_inv[i],':', round(scores[i],5))\n",
    "    words_list.append(vocab_inv[i])\n",
    "    scores_list.append(round(scores[i],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_highlighter(p, ma, mi):\n",
    "    w = p[0]\n",
    "    s = p[1]\n",
    "    cl = ma-mi\n",
    "    g_ = (s-mi)/cl\n",
    "    c_ = ((1-g_)*50)+45\n",
    "    color =  \"hsl(202, 100%, \"+str(c_)+\"%)\"\n",
    "    word = '<span style=\"background-color:' +color+ '\">' +w+ '</span>'\n",
    "    return word\n",
    "\n",
    "text_high = ' '.join([lrp_highlighter(p, max(scores_list),min(scores_list)) for p in zip(words_list, scores_list)])\n",
    "display(HTML(text_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_lrp(case_no):\n",
    "    case = X_vectorized_test[case_no]\n",
    "    scores = np.squeeze(analyzer.analyze(case.toarray()))\n",
    "    e_lrp_str = []\n",
    "    scores = np.abs(scores)\n",
    "    ids = np.flip(scores.argsort()[-10:])\n",
    "    \n",
    "    words_list = []\n",
    "    scores_list = []\n",
    "    for i in ids:\n",
    "        e_lrp_str.append((vocab_inv[i], round(scores[i],5)))\n",
    "        words_list.append(vocab_inv[i])\n",
    "        scores_list.append(round(scores[i],5))\n",
    "        \n",
    "    print(\"WORD CONTRIBUTE:\")\n",
    "    text_high = ' '.join([lrp_highlighter(p, max(scores_list),min(scores_list)) for p in zip(words_list, scores_list)])\n",
    "    display(HTML(text_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cn = -1\n",
    "experimental_data_e_lrp = []\n",
    "for case_no_ in tqdm(random_cases):\n",
    "    cn =cn+1\n",
    "    if (cn>=300) & (cn<=319):\n",
    "        print('id:', cn)\n",
    "        experimental_data_e_lrp.append(e_lrp(case_no_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CounterFactual example</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_case = sentences_test[case_no]\n",
    "sentences_case_word_tokens = word_tokenize(sentences_case)\n",
    "sentences_case_length = len(sentences_case_word_tokens)\n",
    "new_words_count = ceil(sentences_case_length/10)\n",
    "\n",
    "random_words_index = list(np.random.random_integers(0,len(vocab_words)-1,size=[new_words_count,]))\n",
    "new_words = []\n",
    "for i in random_words_index:\n",
    "    new_words.append(vocab_words[i])\n",
    "    \n",
    "random_location_index = list(np.random.random_integers(0,sentences_case_length,size=[new_words_count,]))\n",
    "\n",
    "sentences_new = \"\"\n",
    "for i, w in enumerate(sentences_case_word_tokens):\n",
    "    if i in random_location_index:\n",
    "        sentences_new = sentences_new + \" **\" + new_words[np.where(np.array(random_location_index)==i)[0][0]] + \"**\"\n",
    "    else:\n",
    "        sentences_new = sentences_new + \" \" + w\n",
    "sentences_new = sentences_new.strip()\n",
    "\n",
    "print(sentences_new)\n",
    "text_list = [sentences_new]\n",
    "X_preprocess = []\n",
    "for sen in tqdm(text_list):\n",
    "    X_preprocess.append(preprocess_text(sen))\n",
    "\n",
    "X_vectorized = vectorizer.transform(X_preprocess)\n",
    "\n",
    "clas = model.predict_classes(X_vectorized)\n",
    "print('\\nModel Prediction:', clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_purt_clas(case_no):\n",
    "    sentences_case = sentences_test[case_no]\n",
    "    sentences_case_word_tokens = word_tokenize(sentences_case)\n",
    "    sentences_case_length = len(sentences_case_word_tokens)\n",
    "    new_words_count = ceil(sentences_case_length/10)\n",
    "\n",
    "    random_words_index = list(np.random.random_integers(0,len(vocab_words)-1,size=[new_words_count,]))\n",
    "    new_words = []\n",
    "    for i in random_words_index:\n",
    "        new_words.append(vocab_words[i])\n",
    "\n",
    "    random_location_index = list(np.random.random_integers(0,sentences_case_length,size=[new_words_count,]))\n",
    "\n",
    "    sentences_new = \"\"\n",
    "    for i, w in enumerate(sentences_case_word_tokens):\n",
    "        if i in random_location_index:\n",
    "            sentences_new = sentences_new + \" **\" + new_words[np.where(np.array(random_location_index)==i)[0][0]] + \"**\"\n",
    "        else:\n",
    "            sentences_new = sentences_new + \" \" + w\n",
    "    sentences_new = sentences_new.strip()\n",
    "\n",
    "    text_list = [sentences_new]\n",
    "    X_preprocess = []\n",
    "    for sen in tqdm(text_list):\n",
    "        X_preprocess.append(preprocess_text(sen))\n",
    "\n",
    "    X_vectorized = vectorizer.transform(X_preprocess)\n",
    "\n",
    "    p = model.predict_classes(X_vectorized)[0][0]\n",
    "    \n",
    "    if p == 1:\n",
    "        clas = 'pos'\n",
    "    else:\n",
    "        clas = 'neg'\n",
    "    \n",
    "    return [sentences_new, clas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data_text_purturbed = []\n",
    "experimental_data_y_text_purturbed = []\n",
    "\n",
    "for case_no_ in tqdm(random_cases):\n",
    "    pt, c = text_purt_clas(case_no_)\n",
    "    experimental_data_text_purturbed.append(pt)\n",
    "    experimental_data_y_text_purturbed.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experimental Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for experimental_data_04_02_2021.xlsx 20 cases were used\n",
    "experimental_data = pd.DataFrame()\n",
    "experimental_data['idx'] = experimental_data_idx\n",
    "experimental_data['text'] = experimental_data_text\n",
    "experimental_data['y'] = experimental_data_y\n",
    "experimental_data['y_hat'] = experimental_data_y_hat\n",
    "experimental_data['text_purturbed'] = experimental_data_text_purturbed\n",
    "experimental_data['y_text_purturbed'] = experimental_data_y_text_purturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data.to_csv('experimental_data_04Feb2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
